############################################

# Exemplo de Agente com Streaming de Respostas.
# Demonstra como usar agent.stream() para
# receber chunks de ESTADOS em tempo real,
# mostrando progresso intermedi√°rio durante
# o processamento.
#
# IMPORTANTE: Agents fazem streaming de ESTADOS
# (state chunks), n√£o de tokens individuais.
# Cada chunk representa um passo: pergunta ‚Üí
# tool_call ‚Üí ferramenta ‚Üí resposta final.

############################################


############################################
# PASSO 1 - Definir o prompt do sistema
############################################

SYSTEM_PROMPT = """
Voc√™ √© um assistente inteligente que busca informa√ß√µes e faz an√°lises.

Voc√™ tem acesso √†s seguintes ferramentas:
- buscar_informacoes: Para buscar dados sobre um t√≥pico
- calcular_estatisticas: Para calcular m√©dia, m√≠nimo e m√°ximo de n√∫meros
- gerar_relatorio: Para gerar um relat√≥rio detalhado sobre um tema

Use as ferramentas quando necess√°rio e forne√ßa respostas completas.
"""

############################################
# PASSO 2 - Definir ferramentas
############################################

from langchain.tools import tool
import time


@tool
def buscar_informacoes(query: str) -> str:
    """Buscar informa√ß√µes sobre um t√≥pico."""
    # Simular processamento com delay
    time.sleep(1)

    # Simular base de conhecimento
    info_db = {
        "python": "Python √© uma linguagem de programa√ß√£o de alto n√≠vel, interpretada e de prop√≥sito geral.",
        "ia": "Intelig√™ncia Artificial √© um campo da ci√™ncia da computa√ß√£o focado em criar sistemas que simulam intelig√™ncia humana.",
        "langchain": "LangChain √© um framework para desenvolvimento de aplica√ß√µes com modelos de linguagem.",
    }

    query_lower = query.lower()
    for key, value in info_db.items():
        if key in query_lower:
            return value

    return f"Informa√ß√£o sobre '{query}' n√£o encontrada na base de dados."


@tool
def calcular_estatisticas(numeros: list[float]) -> str:
    """Calcular m√©dia, m√≠nimo e m√°ximo de uma lista de n√∫meros."""
    # Simular processamento
    time.sleep(0.5)

    if not numeros:
        return "Lista vazia fornecida."

    media = sum(numeros) / len(numeros)
    minimo = min(numeros)
    maximo = max(numeros)

    return f"Estat√≠sticas: M√©dia={media:.2f}, M√≠nimo={minimo}, M√°ximo={maximo}"


@tool
def gerar_relatorio(topico: str) -> str:
    """Gerar um relat√≥rio detalhado sobre um tema."""
    # Simular processamento longo
    time.sleep(1.5)

    return f"""
RELAT√ìRIO: {topico.upper()}

Resumo: Este √© um relat√≥rio abrangente sobre {topico}.
An√°lise: Dados indicam crescimento significativo na √°rea.
Conclus√£o: Recomenda-se investimento cont√≠nuo.
"""


############################################
# PASSO 3 - Configurar o modelo
############################################

from langchain.chat_models import init_chat_model
from dotenv import load_dotenv

load_dotenv()

model = init_chat_model(
    "gpt-4o-mini",
    temperature=0.7,
    timeout=20,
    max_tokens=2000,
)

############################################
# PASSO 4 - Criar o agente
############################################

from langchain.agents import create_agent

agent = create_agent(
    model=model,
    system_prompt=SYSTEM_PROMPT,
    tools=[
        buscar_informacoes,
        calcular_estatisticas,
        gerar_relatorio,
    ],
)

############################################
# PASSO 5 - Demonstra√ß√µes pr√°ticas
############################################

print("=" * 70)
print("EXEMPLO 1 - Compara√ß√£o: invoke() vs stream()")
print("=" * 70)

print("\nNOTA: Agents fazem streaming de ESTADOS (state chunks), n√£o de tokens.")
print("Cada chunk representa um passo do agente (pergunta ‚Üí ferramenta ‚Üí resposta).")
print()

print("[A] Usando invoke() - Aguarda resposta completa:")
print("-" * 70)

import time
start_time = time.time()

result = agent.invoke({
    "messages": [{"role": "user", "content": "Busque informa√ß√µes sobre Python"}]
})

elapsed = time.time() - start_time
print(f"Resposta: {result['messages'][-1].content}")
print(f"Tempo total: {elapsed:.2f}s (tudo processado sem feedback intermedi√°rio)")
print()

print("[B] Usando stream() - Mostra progresso em cada passo:")
print("-" * 70)

start_time = time.time()

chunk_count = 0
for chunk in agent.stream({
    "messages": [{"role": "user", "content": "Busque informa√ß√µes sobre Python"}]
}, stream_mode="values"):
    chunk_count += 1
    latest_message = chunk["messages"][-1]
    msg_type = getattr(latest_message, 'type', 'unknown')

    # Mostrar cada passo do processo
    if msg_type == 'human':
        print(f"Chunk {chunk_count}: Pergunta do usu√°rio recebida", flush=True)
    elif hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
        print(f"Chunk {chunk_count}: Agente decidiu chamar ferramentas", flush=True)
    elif msg_type == 'tool':
        print(f"Chunk {chunk_count}: Ferramenta executada", flush=True)
    elif latest_message.content and msg_type == 'ai':
        print(f"Chunk {chunk_count}: Resposta final gerada", flush=True)

elapsed = time.time() - start_time
print(f"Tempo total: {elapsed:.2f}s (com {chunk_count} chunks mostrando progresso)")
print()

print("=" * 70)
print("EXEMPLO 2 - Streaming mostrando cada etapa do processo")
print("=" * 70)

print("\nPergunta: Calcule as estat√≠sticas de [15, 25, 35, 45]")
print("Acompanhando cada etapa:")
print("-" * 70)

for chunk in agent.stream({
    "messages": [{"role": "user", "content": "Calcule as estat√≠sticas de [15, 25, 35, 45]"}]
}, stream_mode="values"):
    latest_message = chunk["messages"][-1]
    msg_type = getattr(latest_message, 'type', 'unknown')

    # Mostrar cada tipo de mensagem que passa pelo stream
    if msg_type == 'human':
        print(f"üì§ Usu√°rio perguntou")
    elif hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
        tools = [tc['name'] for tc in latest_message.tool_calls]
        print(f"ü§î Agente decidiu usar: {', '.join(tools)}")
    elif msg_type == 'tool':
        print(f"‚öôÔ∏è  Ferramenta executada (aguardando...)")
    elif msg_type == 'ai' and latest_message.content:
        print(f"‚úÖ Resposta pronta:")
        print(f"   {latest_message.content[:80]}...")
print()

print("=" * 70)
print("EXEMPLO 3 - Streaming com chamadas de ferramentas")
print("=" * 70)

print("\nPergunta: Busque informa√ß√µes sobre Python e calcule as estat√≠sticas de [10, 20, 30, 40, 50]")
print("Processamento em tempo real:")
print("-" * 70)

for chunk in agent.stream({
    "messages": [{"role": "user", "content": "Busque informa√ß√µes sobre Python e calcule as estat√≠sticas de [10, 20, 30, 40, 50]"}]
}, stream_mode="values"):
    latest_message = chunk["messages"][-1]

    # Detectar quando o agente decide chamar ferramentas
    if hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
        tool_names = [tc['name'] for tc in latest_message.tool_calls]
        print(f"üîß Chamando ferramentas: {', '.join(tool_names)}", flush=True)

    # Mostrar resposta final
    elif latest_message.content and hasattr(latest_message, 'type') and latest_message.type == 'ai':
        print(f"‚úÖ Resposta final:\n{latest_message.content}")

print()

print("=" * 70)
print("EXEMPLO 4 - An√°lise detalhada dos chunks")
print("=" * 70)

print("\nPergunta: Gere um relat√≥rio sobre Intelig√™ncia Artificial")
print("Chunks recebidos (estrutura detalhada):")
print("-" * 70)

chunk_count = 0
for chunk in agent.stream({
    "messages": [{"role": "user", "content": "Gere um relat√≥rio sobre Intelig√™ncia Artificial"}]
}, stream_mode="values"):
    chunk_count += 1
    latest_message = chunk["messages"][-1]

    # Mostrar informa√ß√µes sobre cada chunk
    msg_type = getattr(latest_message, 'type', 'unknown')

    if hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
        print(f"Chunk #{chunk_count}: Tipo={msg_type}, Tool Calls={len(latest_message.tool_calls)}")
        for tc in latest_message.tool_calls:
            print(f"  ‚Üí Ferramenta: {tc['name']}")
    elif latest_message.content:
        content_preview = latest_message.content[:50] + "..." if len(latest_message.content) > 50 else latest_message.content
        print(f"Chunk #{chunk_count}: Tipo={msg_type}, Content='{content_preview}'")

print(f"\nTotal de chunks recebidos: {chunk_count}")
print()

print("=" * 70)
print("EXEMPLO 5 - Stream mode 'values' vs 'updates'")
print("=" * 70)

print("\n[A] Stream mode 'values' (estado completo):")
print("-" * 70)

for i, chunk in enumerate(agent.stream({
    "messages": [{"role": "user", "content": "Calcule as estat√≠sticas de [5, 10, 15]"}]
}, stream_mode="values"), 1):
    total_msgs = len(chunk.get("messages", []))
    print(f"Chunk {i}: Total de mensagens no estado = {total_msgs}")

print()

print("[B] Stream mode 'updates' (apenas mudan√ßas):")
print("-" * 70)

update_count = 0
for chunk in agent.stream({
    "messages": [{"role": "user", "content": "Calcule as estat√≠sticas de [5, 10, 15]"}]
}, stream_mode="updates"):
    update_count += 1
    # Updates mostra apenas o que mudou em cada n√≥ do grafo
    # Cada chave √© um n√≥ que foi executado
    for node_name, node_data in chunk.items():
        if "messages" in node_data:
            num_msgs = len(node_data["messages"])
            print(f"Update {update_count}: N√≥ '{node_name}' adicionou {num_msgs} mensagem(s)")
        else:
            print(f"Update {update_count}: N√≥ '{node_name}' executado")

if update_count == 0:
    print("(Nenhum update recebido - pode variar conforme vers√£o do LangGraph)")

print()

print("=" * 70)
print("EXEMPLO 6 - Indicador de progresso visual")
print("=" * 70)

print("\nPergunta: Busque sobre LangChain e gere um relat√≥rio")
print("Progresso:")
print("-" * 70)

import sys

steps = ["Iniciando", "Processando", "Gerando resposta"]
step_index = 0

for chunk in agent.stream({
    "messages": [{"role": "user", "content": "Busque sobre LangChain e gere um relat√≥rio"}]
}, stream_mode="values"):
    latest_message = chunk["messages"][-1]

    if hasattr(latest_message, 'tool_calls') and latest_message.tool_calls:
        # Mostrar indicador de progresso
        tool_names = [tc['name'] for tc in latest_message.tool_calls]
        print(f"‚è≥ Executando: {', '.join(tool_names)}...", end='', flush=True)
        print(" ‚úì")

    elif latest_message.content and hasattr(latest_message, 'type') and latest_message.type == 'ai':
        print(f"‚úÖ Conclu√≠do!\n")
        # Mostrar apenas parte da resposta
        content_lines = latest_message.content.split('\n')
        print("Resposta (primeiras linhas):")
        for line in content_lines[:3]:
            if line.strip():
                print(f"  {line}")

print()

############################################
# OBSERVA√á√ïES IMPORTANTES
############################################

print("=" * 70)
print("OBSERVA√á√ïES IMPORTANTES")
print("=" * 70)
print("""
1. CONCEITO IMPORTANTE - STREAMING DE ESTADOS:
   - Agents fazem streaming de ESTADOS, n√£o de tokens individuais
   - Cada chunk = um PASSO do agente (pergunta ‚Üí tool_call ‚Üí resultado ‚Üí resposta)
   - Diferente do ChatGPT que mostra palavra por palavra (streaming de tokens)
   - Exemplo: Pergunta com ferramenta gera 4 chunks (human ‚Üí ai tool_call ‚Üí tool ‚Üí ai response)

2. INVOKE() VS STREAM():
   - invoke(): Aguarda processamento completo, retorna tudo de uma vez
   - stream(): Retorna chunks incrementais a cada PASSO do processo
   - stream() permite mostrar progresso e melhorar UX

3. STREAM MODES:
   - "values": Cada chunk cont√©m o ESTADO COMPLETO at√© aquele ponto
   - "updates": Cada chunk cont√©m apenas o que MUDOU
   - "messages": Stream apenas as mensagens (modo legado)
   - Recomendado: Use "values" para maioria dos casos

4. ESTRUTURA DOS CHUNKS:
   - chunk["messages"]: Lista de todas as mensagens at√© o momento
   - chunk["messages"][-1]: √öltima mensagem (mais recente)
   - latest_message.content: Texto da resposta (se houver)
   - latest_message.tool_calls: Ferramentas sendo chamadas (se houver)

5. DETECTAR TIPO DE MENSAGEM:
   - latest_message.type == 'ai': Mensagem do assistente
   - latest_message.type == 'tool': Resultado de ferramenta
   - latest_message.type == 'human': Mensagem do usu√°rio
   - hasattr(msg, 'tool_calls'): Verificar se est√° chamando ferramentas

6. CASOS DE USO PARA STREAMING DE ESTADOS:
   - UI responsiva: Mostrar "pensando..." ou spinner a cada passo
   - Feedback ao usu√°rio: "Buscando informa√ß√µes..." quando tool √© chamada
   - Progresso visual: Barra de progresso baseada em etapas
   - M√∫ltiplas ferramentas: Indicar qual ferramenta est√° sendo executada
   - Log em tempo real: Registrar cada a√ß√£o do agente

7. PERFORMANCE E UX:
   - Streaming melhora percep√ß√£o de velocidade
   - Usu√°rio v√™ progresso imediato a cada passo
   - Especialmente √∫til com ferramentas lentas (API calls, DB queries)
   - Use flush=True para output imediato no terminal

8. TRATAMENTO EM PRODU√á√ÉO:
   - Sempre use try/except ao iterar chunks
   - Trate poss√≠veis timeouts ou erros de rede
   - Considere buffer para evitar muitos updates pequenos
   - Implemente retry logic se necess√°rio

9. LIMITA√á√ïES E DIFEREN√áAS:
   - Streaming de estados ‚â† Streaming de tokens
   - N√£o mostra texto sendo "digitado" palavra por palavra
   - Streaming n√£o reduz tempo total de processamento
   - Apenas melhora experi√™ncia do usu√°rio mostrando progresso
   - Chunks podem chegar em ordens diferentes com async

10. STREAMING DE TOKENS (ALTERNATIVA):
    - Para streaming palavra-por-palavra, use model.stream() diretamente
    - N√£o funciona bem com agents que usam ferramentas
    - Exemplo: model.stream("pergunta") retorna tokens incrementais
    - Agents (create_agent) fazem streaming de ESTADOS, n√£o tokens

11. PR√ìXIMOS PASSOS:
   - Para combinar streaming com mem√≥ria, veja sample008.py
   - Para tratamento de erros, veja sample011.py
   - Para estado customizado, veja sample016.py e sample017.py
""")
